{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkM6nIoV7YStJcOsdpQT8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/YOLACT_Mobius/blob/main/YOLOv8_inference_LR_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mobius YOLO8 inference (左右判定あり)**\n",
        "\n",
        "```\n",
        "# Classes\n",
        "names:\n",
        "  0: eyelid_R\n",
        "  1: iris_R\n",
        "  2: pupil_R\n",
        "  3: eyelid_L\n",
        "  4: iris_L\n",
        "  5: pupil_L\n",
        "```"
      ],
      "metadata": {
        "id": "ea9PTEhEBYS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "#driveのマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/ultralytics/ultralytics\n",
        "%cd ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t8zQCmuxQT7",
        "outputId": "3faeac56-6e43-4c53-8ce2-14220f235ebf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 43797, done.\u001b[K\n",
            "remote: Counting objects: 100% (1608/1608), done.\u001b[K\n",
            "remote: Compressing objects: 100% (987/987), done.\u001b[K\n",
            "remote: Total 43797 (delta 1094), reused 989 (delta 611), pack-reused 42189 (from 1)\u001b[K\n",
            "Receiving objects: 100% (43797/43797), 38.44 MiB | 21.29 MiB/s, done.\n",
            "Resolving deltas: 100% (32394/32394), done.\n",
            "/content/ultralytics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unilateral model"
      ],
      "metadata": {
        "id": "PiA9778w1yQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#yolov8\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/MOBIUS/YOLOv8_training_LR/ultralytics/runs/segment/train4/weights/best.pt\")\n",
        "#model = YOLO(\"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/MOBIUS/YOLOv8_training/ultralytics/runs/segment/train3_458epochs/weights/best.pt\")\n",
        "\n",
        "\n",
        "# # ディレクトリパスを設定\n",
        "# image_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/images/val\"\n",
        "# # ディレクトリ内の画像ファイルを取得\n",
        "# image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "# img_path = image_files[13]\n",
        "\n",
        "img_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/10.jpg\"\n",
        "\n",
        "results = model(img_path, save=False, save_txt=False)  # predict on an image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL_CXQnxxQWw",
        "outputId": "0e139063-2c1a-4029-a2f7-89cbf55c584d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/10.jpg: 640x640 1 eyelid, 1 iris, 1 pupil, 532.3ms\n",
            "Speed: 3.1ms preprocess, 532.3ms inference, 17.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 元のクラスをマージ後のクラスにマッピング\n",
        "merged_classes = {\n",
        "    0: \"eyelid\",  # クラス0を新しいクラス0に\n",
        "    1: \"iris\",  # クラス1を新しいクラス0にマージ\n",
        "    2: \"pupil\",   # クラス2は新しいクラス1として保持\n",
        "}\n",
        "\n",
        "# クラス名ごとの色を定義\n",
        "class_colors = {\n",
        "    \"eyelid\": (255, 0, 0),   # 赤\n",
        "    \"iris\": (0, 255, 0),     # 緑\n",
        "    \"pupil\": (0, 0, 255),    # 青\n",
        "}\n"
      ],
      "metadata": {
        "id": "3qymkaXF0zza"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# クラスインデックスをクラス名にマッピング\n",
        "merged_classes = {\n",
        "    0: \"eyelid\",\n",
        "    1: \"iris\",\n",
        "    2: \"pupil\",\n",
        "}\n",
        "\n",
        "# クラス名ごとの色を定義\n",
        "class_colors = {\n",
        "    \"eyelid\": (255, 0, 0),   # 赤\n",
        "    \"iris\": (0, 255, 0),     # 緑\n",
        "    \"pupil\": (0, 0, 255),    # 青\n",
        "}\n",
        "\n",
        "# クラス名のリストを取得\n",
        "unique_classes = list(set(merged_classes.values()))\n",
        "\n",
        "# 画像を読み込む\n",
        "img = cv2.imread(img_path)\n",
        "if img is None:\n",
        "    raise ValueError(f\"画像が見つかりませんでした: {img_path}\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# 結果を取得\n",
        "result = results[0]\n",
        "\n",
        "# マスクとボックスを取得\n",
        "masks = result.masks.data.cpu().numpy()  # マスクのリスト\n",
        "boxes = result.boxes.data.cpu().numpy()  # ボックスのリスト\n",
        "\n",
        "# セグメンテーションマスクを重ねる\n",
        "for i, mask in enumerate(masks):\n",
        "    original_cls = int(boxes[i, 5])  # 元のクラスインデックス\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")  # クラス名を取得\n",
        "    color = class_colors.get(class_name, (255, 255, 255))  # クラス名に対応する色を取得（デフォルトは白）\n",
        "\n",
        "    mask = mask.astype(np.uint8) * 255\n",
        "\n",
        "    # マスクを元の画像のサイズにリサイズ\n",
        "    mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "    colored_mask = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "    colored_mask[mask > 0] = color\n",
        "\n",
        "    # マスクを画像にブレンド (50%の透過度)\n",
        "    img = cv2.addWeighted(img, 1, colored_mask, 0.5, 0)\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "for box in boxes:\n",
        "    x1, y1, x2, y2 = map(int, box[:4])\n",
        "    original_cls = int(box[5])\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")\n",
        "    color = class_colors.get(class_name, (255, 255, 255))\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "    # クラス名をバウンディングボックスに表示\n",
        "    cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9, color, 2, cv2.LINE_AA)\n",
        "\n",
        "# 凡例の作成\n",
        "legend_patches = [mpatches.Patch(color=np.array(color)/255, label=cls) for cls, color in class_colors.items()]\n",
        "\n",
        "# 結果を表示\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.legend(handles=legend_patches, loc='upper right')\n",
        "plt.title('YOLOv8 Segmentation Result with Merged Classes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eQytleI1xQbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bilateral model"
      ],
      "metadata": {
        "id": "GIrSLeZl12HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#yolov8\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/MOBIUS/YOLOv8_training_LR/ultralytics/runs/segment/train4/weights/best.pt\")\n",
        "\n",
        "img_path = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/15.jpg\"\n",
        "\n",
        "results = model(img_path, save=False, save_txt=False)  # predict on an image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZDb885K1-dg",
        "outputId": "1af72c38-479d-45b0-d9a8-49dc2b2cbc19"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/15.jpg: 640x640 1 eyelid, 1 iris, 1 pupil, 241.6ms\n",
            "Speed: 5.0ms preprocess, 241.6ms inference, 16.2ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# 元のクラスをマージ後のクラスにマッピング\n",
        "merged_classes = {\n",
        "    0: \"eyelid_L\",\n",
        "    1: \"iris_L\",\n",
        "    2: \"pupil_L\",\n",
        "    3: \"eyelid_R\",\n",
        "    4: \"iris_R\",\n",
        "    5: \"pupil_R\",\n",
        "}\n",
        "\n",
        "# クラス名ごとの色を定義\n",
        "class_colors = {\n",
        "    \"eyelid_L\": (255, 0, 0),   # 赤\n",
        "    \"iris_L\": (0, 255, 0),     # 緑\n",
        "    \"pupil_L\": (0, 0, 255),    # 青\n",
        "    \"eyelid_R\": (255, 0, 0),   # 赤\n",
        "    \"iris_R\": (0, 255, 0),     # 緑\n",
        "    \"pupil_R\": (0, 0, 255),    # 青\n",
        "}\n",
        "\n",
        "# クラス名のリストを取得\n",
        "unique_classes = list(set(merged_classes.values()))\n",
        "\n",
        "# 画像を読み込む\n",
        "img = cv2.imread(img_path)\n",
        "if img is None:\n",
        "    raise ValueError(f\"画像が見つかりませんでした: {img_path}\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# 結果を取得\n",
        "result = results[0]\n",
        "\n",
        "# マスクとボックスを取得\n",
        "masks = result.masks.data.cpu().numpy()  # マスクのリスト\n",
        "boxes = result.boxes.data.cpu().numpy()  # ボックスのリスト\n",
        "\n",
        "# セグメンテーションマスクを重ねる\n",
        "for i, mask in enumerate(masks):\n",
        "    original_cls = int(boxes[i, 5])  # 元のクラスインデックス\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")  # クラス名を取得\n",
        "    color = class_colors.get(class_name, (255, 255, 255))  # クラス名に対応する色を取得（デフォルトは白）\n",
        "\n",
        "    mask = mask.astype(np.uint8) * 255\n",
        "\n",
        "    # マスクを元の画像のサイズにリサイズ\n",
        "    mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
        "\n",
        "    colored_mask = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
        "    colored_mask[mask > 0] = color\n",
        "\n",
        "    # マスクを画像にブレンド (50%の透過度)\n",
        "    img = cv2.addWeighted(img, 1, colored_mask, 0.5, 0)\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "for box in boxes:\n",
        "    x1, y1, x2, y2 = map(int, box[:4])\n",
        "    original_cls = int(box[5])\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")\n",
        "    color = class_colors.get(class_name, (255, 255, 255))\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "    # クラス名をバウンディングボックスに表示\n",
        "    cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9, color, 2, cv2.LINE_AA)\n",
        "\n",
        "# 凡例の作成\n",
        "legend_patches = [mpatches.Patch(color=np.array(color)/255, label=cls) for cls, color in class_colors.items()]\n",
        "\n",
        "# 結果を表示\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.legend(handles=legend_patches, loc='upper right')\n",
        "plt.title('YOLOv8 Segmentation Result with Merged Classes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0JpqVAVr1-ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yolov8\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# モデルのロード\n",
        "model = YOLO(\"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/MOBIUS/YOLOv8_training_LR/ultralytics/runs/segment/train4/weights/best.pt\")\n",
        "\n",
        "# 画像のパス\n",
        "img_path = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/images/val/3_1i_Ls_2.jpg\"\n",
        "\n",
        "# 推論の実行\n",
        "results = model(img_path, save=False, save_txt=False)  # 画像に対して予測を実行\n",
        "\n",
        "# 元のクラスをマージ後のクラスにマッピング\n",
        "merged_classes = {\n",
        "    0: \"eyelid_L\",\n",
        "    1: \"iris_L\",\n",
        "    2: \"pupil_L\",\n",
        "    3: \"eyelid_R\",\n",
        "    4: \"iris_R\",\n",
        "    5: \"pupil_R\",\n",
        "}\n",
        "\n",
        "# クラス名ごとの色を定義\n",
        "class_colors = {\n",
        "    \"eyelid_L\": (255, 0, 0),   # 赤\n",
        "    \"iris_L\": (0, 255, 0),     # 緑\n",
        "    \"pupil_L\": (0, 0, 255),    # 青\n",
        "    \"eyelid_R\": (255, 0, 0),   # 赤\n",
        "    \"iris_R\": (0, 255, 0),     # 緑\n",
        "    \"pupil_R\": (0, 0, 255),    # 青\n",
        "}\n",
        "\n",
        "# 画像を読み込む\n",
        "img = cv2.imread(img_path)\n",
        "if img is None:\n",
        "    raise ValueError(f\"画像が見つかりませんでした: {img_path}\")\n",
        "original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 元画像を保持\n",
        "processed_img = original_img.copy()  # セグメンテーション結果用にコピー\n",
        "\n",
        "# 結果を取得\n",
        "result = results[0]\n",
        "\n",
        "# マスクとボックスを取得\n",
        "masks = result.masks.data.cpu().numpy()  # マスクのリスト\n",
        "boxes = result.boxes.data.cpu().numpy()  # ボックスのリスト\n",
        "\n",
        "# セグメンテーションマスクを重ねる\n",
        "for i, mask in enumerate(masks):\n",
        "    original_cls = int(boxes[i, 5])  # 元のクラスインデックス\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")  # クラス名を取得\n",
        "    color = class_colors.get(class_name, (255, 255, 255))  # クラス名に対応する色を取得（デフォルトは白）\n",
        "\n",
        "    mask = mask.astype(np.uint8) * 255\n",
        "\n",
        "    # マスクを元の画像のサイズにリサイズ（必要に応じて）\n",
        "    mask = cv2.resize(mask, (processed_img.shape[1], processed_img.shape[0]))\n",
        "\n",
        "    colored_mask = np.zeros((processed_img.shape[0], processed_img.shape[1], 3), dtype=np.uint8)\n",
        "    colored_mask[mask > 0] = color\n",
        "\n",
        "    # マスクを画像にブレンド (50%の透過度)\n",
        "    processed_img = cv2.addWeighted(processed_img, 1, colored_mask, 0.5, 0)\n",
        "\n",
        "# バウンディングボックスを描画\n",
        "for box in boxes:\n",
        "    x1, y1, x2, y2 = map(int, box[:4])\n",
        "    original_cls = int(box[5])\n",
        "    class_name = merged_classes.get(original_cls, \"unknown\")\n",
        "    color = class_colors.get(class_name, (255, 255, 255))\n",
        "    cv2.rectangle(processed_img, (x1, y1), (x2, y2), color, 2)\n",
        "    # クラス名をバウンディングボックスに表示\n",
        "    cv2.putText(processed_img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9, color, 2, cv2.LINE_AA)\n",
        "\n",
        "# 凡例の作成\n",
        "# クラス名と色のペアで重複を避けるためにsetを使用\n",
        "unique_class_colors = {cls: color for cls, color in class_colors.items()}\n",
        "legend_patches = [mpatches.Patch(color=np.array(color)/255, label=cls) for cls, color in unique_class_colors.items()]\n",
        "\n",
        "# 元画像と推論画像を左右に並べて表示\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
        "\n",
        "# 元画像の表示\n",
        "axes[0].imshow(original_img)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('元画像')\n",
        "\n",
        "# 推論結果の表示\n",
        "axes[1].imshow(processed_img)\n",
        "axes[1].axis('off')\n",
        "axes[1].set_title('YOLOv8 Segmentation Result with Merged Classes')\n",
        "axes[1].legend(handles=legend_patches, loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NuMAuHRS1-nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリのインポート\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import os\n",
        "\n",
        "# モデルのロード\n",
        "model_path = \"/content/drive/MyDrive/Deep_learning/Eyelid_segmentation/MOBIUS/YOLOv8_training_LR/ultralytics/runs/segment/train4/weights/best.pt\"\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# 画像ディレクトリの設定\n",
        "image_dir = \"/content/drive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/\"\n",
        "\n",
        "# 処理する画像の番号リスト\n",
        "image_numbers = range(1, 21)  # 1 から 20 まで\n",
        "\n",
        "# 元のクラスをマージ後のクラスにマッピング\n",
        "merged_classes = {\n",
        "    0: \"eyelid_L\",\n",
        "    1: \"iris_L\",\n",
        "    2: \"pupil_L\",\n",
        "    3: \"eyelid_R\",\n",
        "    4: \"iris_R\",\n",
        "    5: \"pupil_R\",\n",
        "}\n",
        "\n",
        "# クラス名ごとの色を定義\n",
        "class_colors = {\n",
        "    \"eyelid_L\": (255, 0, 0),   # 赤\n",
        "    \"iris_L\": (0, 255, 0),     # 緑\n",
        "    \"pupil_L\": (0, 0, 255),    # 青\n",
        "    \"eyelid_R\": (255, 0, 0),   # 赤\n",
        "    \"iris_R\": (0, 255, 0),     # 緑\n",
        "    \"pupil_R\": (0, 0, 255),    # 青\n",
        "}\n",
        "\n",
        "# クラス名のリストを取得\n",
        "unique_classes = list(set(merged_classes.values()))\n",
        "\n",
        "# 凡例の作成（クラス名と色のペアで重複を避けるためにsetを使用）\n",
        "unique_class_colors = {cls: color for cls, color in class_colors.items()}\n",
        "legend_patches = [mpatches.Patch(color=np.array(color)/255, label=cls) for cls, color in unique_class_colors.items()]\n",
        "\n",
        "# 各画像に対して処理を実行\n",
        "for num in image_numbers:\n",
        "    # 画像ファイル名とパスの作成\n",
        "    image_filename = f\"{num}.jpg\"\n",
        "    img_path = os.path.join(image_dir, image_filename)\n",
        "\n",
        "    # 画像の存在確認\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"画像が見つかりませんでした: {img_path}\")\n",
        "        continue  # 次の画像へスキップ\n",
        "\n",
        "    # 推論の実行\n",
        "    results = model(img_path, save=False, save_txt=False)  # 画像に対して予測を実行\n",
        "\n",
        "    # 画像を読み込む\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"画像を読み込めませんでした: {img_path}\")\n",
        "        continue  # 次の画像へスキップ\n",
        "    original_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 元画像を保持\n",
        "    processed_img = original_img.copy()  # セグメンテーション結果用にコピー\n",
        "\n",
        "    # 結果を取得\n",
        "    result = results[0]\n",
        "\n",
        "    # マスクとボックスを取得\n",
        "    masks = result.masks.data.cpu().numpy()  # マスクのリスト\n",
        "    boxes = result.boxes.data.cpu().numpy()  # ボックスのリスト\n",
        "\n",
        "    # セグメンテーションマスクを重ねる\n",
        "    for i, mask in enumerate(masks):\n",
        "        if i >= len(boxes):\n",
        "            print(f\"マスクの数がボックスの数を超えています: {img_path}\")\n",
        "            break  # マスク数がボックス数を超えた場合、ループを抜ける\n",
        "\n",
        "        original_cls = int(boxes[i, 5])  # 元のクラスインデックス\n",
        "        class_name = merged_classes.get(original_cls, \"unknown\")  # クラス名を取得\n",
        "        color = class_colors.get(class_name, (255, 255, 255))  # クラス名に対応する色を取得（デフォルトは白）\n",
        "\n",
        "        mask = mask.astype(np.uint8) * 255\n",
        "\n",
        "        # マスクを元の画像のサイズにリサイズ（必要に応じて）\n",
        "        mask = cv2.resize(mask, (processed_img.shape[1], processed_img.shape[0]))\n",
        "\n",
        "        colored_mask = np.zeros((processed_img.shape[0], processed_img.shape[1], 3), dtype=np.uint8)\n",
        "        colored_mask[mask > 0] = color\n",
        "\n",
        "        # マスクを画像にブレンド (50%の透過度)\n",
        "        processed_img = cv2.addWeighted(processed_img, 1, colored_mask, 0.5, 0)\n",
        "\n",
        "    # バウンディングボックスを描画\n",
        "    for box in boxes:\n",
        "        x1, y1, x2, y2 = map(int, box[:4])\n",
        "        original_cls = int(box[5])\n",
        "        class_name = merged_classes.get(original_cls, \"unknown\")\n",
        "        color = class_colors.get(class_name, (255, 255, 255))\n",
        "        cv2.rectangle(processed_img, (x1, y1), (x2, y2), color, 2)\n",
        "        # クラス名をバウンディングボックスに表示\n",
        "        cv2.putText(processed_img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.9, color, 2, cv2.LINE_AA)\n",
        "\n",
        "    # 比較用のプロットを作成\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(24, 12))\n",
        "\n",
        "    # 元画像の表示\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].axis('off')\n",
        "    axes[0].set_title(f'元画像: {image_filename}', fontsize=16)\n",
        "\n",
        "    # 推論結果の表示\n",
        "    axes[1].imshow(processed_img)\n",
        "    axes[1].axis('off')\n",
        "    axes[1].set_title(f'YOLOv8 Segmentation Result: {image_filename}', fontsize=16)\n",
        "    axes[1].legend(handles=legend_patches, loc='upper right', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 比較画像の表示\n",
        "    plt.show()\n",
        "    plt.close(fig)  # メモリ節約のために図を閉じる\n",
        "\n",
        "    print(f\"処理完了: {image_filename}\")\n",
        "\n",
        "print(\"全ての画像の処理が完了しました。\")\n"
      ],
      "metadata": {
        "id": "S6Sa39hc4t3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBu7cOCV4t6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUItjtQR4t8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFOG1v-xwOKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EW8TExXwOM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqI2XtP6wH0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}