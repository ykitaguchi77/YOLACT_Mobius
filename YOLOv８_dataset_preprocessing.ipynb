{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfM8RJojxvcgMVZ7tuP/w+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/YOLACT_Mobius/blob/main/YOLOv%EF%BC%98_dataset_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mobius_dataset --> YOLOv8 instance segmentation training**\n",
        "\n",
        "2024研究室配属（鈴木くん、北口くん）\n",
        "\n",
        "```\n",
        "To do：\n",
        "\n",
        "Mobiusのマスク画像をYOLOv8形式のテキストファイルに\n",
        "\n",
        "マスク画像がない画像を除外\n",
        "\n",
        "YOLOv8用のフォルダ構成に\n",
        "```"
      ],
      "metadata": {
        "id": "UO-YiNInXIqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqgoOYBDXINA",
        "outputId": "3ef0c9e8-e4e8-41c3-a2f4-aeb98a7f9751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**COCO_json形式への変換**"
      ],
      "metadata": {
        "id": "KyVVyl1QgCtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from skimage import measure\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 画像から特定の色のマスクを作成する\n",
        "def create_mask(image, color):\n",
        "    return np.all(image == color, axis=-1)\n",
        "\n",
        "# マスクを適用して画像を変換する\n",
        "def apply_mask(image, mask):\n",
        "    return np.where(mask, 255, 0)\n",
        "\n",
        "# 画像から輪郭を抽出し、点のリストを生成する\n",
        "def extract_contours(img, _num_points):\n",
        "    contours = measure.find_contours(img, 0.5)\n",
        "    points_list = []\n",
        "\n",
        "    for contour in contours:\n",
        "        contour_length = len(contour)\n",
        "\n",
        "        if contour_length > _num_points:\n",
        "            step = contour_length // _num_points\n",
        "            selected_points = [contour[i * step] for i in range(_num_points)]\n",
        "        else:\n",
        "            selected_points = contour\n",
        "\n",
        "        points = [[float(point[1]), float(point[0])] for point in selected_points]\n",
        "        points_list.append(points)\n",
        "\n",
        "    return points_list\n",
        "\n",
        "# 点のリストからJSONファイルを作成する\n",
        "def create_json(points_list_blue, points_list_blue_green, points_list_all_colors, json_name, img_path, img_data, img_height, img_width):\n",
        "    data = {\n",
        "        \"version\": \"5.4.1\",\n",
        "        \"flags\": {},\n",
        "        \"shapes\": [],\n",
        "        \"imagePath\": img_path,\n",
        "        \"imageData\": img_data,\n",
        "        \"imageHeight\": img_height,\n",
        "        \"imageWidth\": img_width\n",
        "    }\n",
        "\n",
        "    # 点のデータをJSON構造に追加\n",
        "    data[\"shapes\"].append({\n",
        "        \"label\": \"eyelid\",\n",
        "        \"points\": points_list_all_colors,\n",
        "        \"group_id\": None,\n",
        "        \"description\": \"\",\n",
        "        \"shape_type\": \"polygon\",\n",
        "        \"flags\": {},\n",
        "        \"mask\": None\n",
        "    })\n",
        "\n",
        "    data[\"shapes\"].append({\n",
        "        \"label\": \"iris\",\n",
        "        \"points\": points_list_blue_green,\n",
        "        \"group_id\": None,\n",
        "        \"description\": \"\",\n",
        "        \"shape_type\": \"polygon\",\n",
        "        \"flags\": {},\n",
        "        \"mask\": None\n",
        "    })\n",
        "\n",
        "    data[\"shapes\"].append({\n",
        "        \"label\": \"pupil\",\n",
        "        \"points\": points_list_blue,\n",
        "        \"group_id\": None,\n",
        "        \"description\": \"\",\n",
        "        \"shape_type\": \"polygon\",\n",
        "        \"flags\": {},\n",
        "        \"mask\": None\n",
        "    })\n",
        "\n",
        "    with open(json_name, 'w') as fw:\n",
        "        json.dump(data, fw, indent=2)\n",
        "\n",
        "# 画像からマスクを作成し、輪郭を抽出してJSONファイルに保存する\n",
        "def process_single_image(mask_file, json_name):\n",
        "    img = cv2.imread(mask_file, cv2.IMREAD_COLOR)\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    mask_blue = create_mask(img, [255, 0, 0])\n",
        "    mask_green = create_mask(img, [0, 255, 0])\n",
        "    mask_red = create_mask(img, [0, 0, 255])\n",
        "\n",
        "    mask_blue_green = np.logical_or(mask_blue, mask_green)\n",
        "    mask_all_colors = np.logical_or(mask_blue_green, mask_red)\n",
        "\n",
        "    img_blue = apply_mask(img, mask_blue)\n",
        "    img_blue_green = apply_mask(img, mask_blue_green)\n",
        "    img_all_colors = apply_mask(img, mask_all_colors)\n",
        "\n",
        "    points_list_blue = extract_contours(img_blue, num_points)\n",
        "    points_list_blue_green = extract_contours(img_blue_green, num_points)\n",
        "    points_list_all_colors = extract_contours(img_all_colors, num_points)\n",
        "\n",
        "    create_json(points_list_blue, points_list_blue_green, points_list_all_colors, json_name, mask_file, \"/9\", img_height, img_width)\n",
        "\n",
        "# ディレクトリ内の全画像に対して上記の処理をまとめて行う\n",
        "def process_images(mask_dir, output_json_dir, num_images=10):\n",
        "    if not os.path.exists(mask_dir):\n",
        "        raise FileNotFoundError(f\"The directory {mask_dir} does not exist.\")\n",
        "\n",
        "    if not os.path.exists(output_json_dir):\n",
        "        user_input = input(f\"The directory {output_json_dir} does not exist. Do you want to create it? (y/n): \")\n",
        "        if user_input.lower() == 'y':\n",
        "            os.makedirs(output_json_dir)\n",
        "            print(f\"Created directory {output_json_dir}\")\n",
        "        else:\n",
        "            print(\"Directory not created. Exiting the process.\")\n",
        "            return\n",
        "\n",
        "    total_images = sum([len(files) for r, d, files in os.walk(mask_dir) if files])\n",
        "\n",
        "    with tqdm(total=min(total_images, num_images)) as pbar:\n",
        "        for folder in os.listdir(mask_dir):\n",
        "            folder_path = os.path.join(mask_dir, folder)\n",
        "            if not os.path.isdir(folder_path):\n",
        "                continue\n",
        "\n",
        "            output_folder = os.path.join(output_json_dir, folder)\n",
        "            os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "            for filename in os.listdir(folder_path):\n",
        "                if filename.endswith(\".png\"):\n",
        "                    mask_file = os.path.join(folder_path, filename)\n",
        "                    json_name = os.path.join(output_folder, filename.replace(\".png\", \".json\"))\n",
        "\n",
        "                    process_single_image(mask_file, json_name)\n",
        "\n",
        "                    pbar.update(1)\n",
        "                    if pbar.n >= num_images:\n",
        "                        return\n",
        "\n"
      ],
      "metadata": {
        "id": "FJ82CD49EZuI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#マスクされた画像のパスを指定してください。\n",
        "mask_file = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/masks/1/1_1i_Ll_1.png\"\n",
        "json_name = \"/content/output.json\"\n",
        "\n",
        "#輪郭点の個数を指定してください。\n",
        "num_points = 24\n",
        "\n",
        "process_single_image(mask_file, json_name)\n",
        "\n",
        "# 元画像と輪郭の比較\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False\n",
        "with open('/content/output.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "points_eyelid = []\n",
        "points_iris = []\n",
        "points_pupil = []\n",
        "for shape in data['shapes']:\n",
        "    if shape['label'] == 'eyelid':\n",
        "        points_eyelid += shape['points'][0]\n",
        "    elif shape['label'] == 'iris':\n",
        "        points_iris += shape['points'][0]\n",
        "    elif shape['label'] == 'pupil':\n",
        "        points_pupil += shape['points'][0]\n",
        "\n",
        "\n",
        "x_eyelid = [point[0] for point in points_eyelid]\n",
        "y_eyelid = [point[1] for point in points_eyelid]\n",
        "\n",
        "x_iris = [point[0] for point in points_iris]\n",
        "y_iris = [point[1] for point in points_iris]\n",
        "\n",
        "x_pupil = [point[0] for point in points_pupil]\n",
        "y_pupil = [point[1] for point in points_pupil]\n",
        "\n",
        "# 元の画像を読み込む\n",
        "img_path = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/images/1/1_1i_Ll_1.jpg\"\n",
        "img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "# 図の左隣に元の画像を表示する\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(img_rgb)\n",
        "plt.title(\"original\")\n",
        "\n",
        "# 図の右隣に散布図を表示する\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(x_eyelid, y_eyelid, label='eyelid')\n",
        "plt.scatter(x_iris, y_iris, label='iris')\n",
        "plt.scatter(x_pupil, y_pupil, label='pupil')\n",
        "plt.title(\"contour\")#輪郭\n",
        "\n",
        "# 縦横の縮尺を同じにする\n",
        "plt.axis('equal')\n",
        "\n",
        "# 縦軸を降順にする\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# 凡例を表示する\n",
        "plt.legend()\n",
        "\n",
        "# 図を表示する\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrryMPq_aYmb",
        "outputId": "796c583c-484f-4191-c83c-4802c75bbf93"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-e6a460a0f4bc>:43: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
            "  plt.subplot(1, 2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 基本ディレクトリと出力ディレクトリの設定\n",
        "mask_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/masks\"\n",
        "output_json_dir = \"/content/output_json\""
      ],
      "metadata": {
        "id": "4Ps2mN9fYgFc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初は10枚の画像でテスト\n",
        "process_images(mask_dir, output_json_dir, num_images=10)\n",
        "\n",
        "# 問題がなければ全画像に適用\n",
        "# process_images(base_dir, output_dir, num_images=float('inf'))"
      ],
      "metadata": {
        "id": "UOYhHI09YkwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e40e5ba-e21c-4773-fc56-884e084e3f63"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory /content/output_json does not exist. Do you want to create it? (y/n): y\n",
            "Created directory /content/output_json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def process_all_json_files(json_dir, num_images=10):\n",
        "    # JSONファイルの総数を取得\n",
        "    total_files = sum([len(files) for r, d, files in os.walk(json_dir) if files])\n",
        "\n",
        "    # tqdmを使って進捗バーを初期化\n",
        "    with tqdm(total=min(total_files, num_images)) as pbar:\n",
        "        for root, dirs, files in os.walk(json_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".json\"):\n",
        "                    json_file_path = os.path.join(root, file)\n",
        "\n",
        "                    # JSONファイルを読み込み\n",
        "                    with open(json_file_path, 'r') as json_file:\n",
        "                        data = json.load(json_file)\n",
        "\n",
        "                    # imagePathの値をファイル名のみに更新\n",
        "                    data[\"imagePath\"] = os.path.basename(data[\"imagePath\"])\n",
        "\n",
        "                    # JSONファイルに変更を保存\n",
        "                    with open(json_file_path, 'w') as json_file:\n",
        "                        json.dump(data, json_file, indent=4)\n",
        "\n",
        "                    pbar.update(1)  # 進捗バーを更新\n",
        "                    if pbar.n >= num_images:\n",
        "                        return  # 指定された数のファイルを処理したら終了\n",
        "\n",
        "# JSONディレクトリのパス\n",
        "json_dir = \"/content/output_json\"\n",
        "\n",
        "# 最初は10個のファイルでテスト\n",
        "process_all_json_files(json_dir, num_images=10)\n",
        "\n",
        "# 問題がなければ、全ファイルに適用\n",
        "#process_all_json_files(json_dir, num_images=float('inf'))\n"
      ],
      "metadata": {
        "id": "j5n3-ew1YoeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed980a33-8a5c-43c9-ec01-bfbaefcc5aac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1339.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: jsonフォルダ内のフォルダに入っている全てのファイルをバラバラの状態で一つ一つ、/content/drive/MyDrive/AI_laboratory_course/MOBIUS/json_concatにコピーを作成してください\n",
        "\n",
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# source_dir = \"/content/output_json\"\n",
        "# target_dir = \"/content/concat_json\"\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "# if not os.path.exists(target_dir):\n",
        "#     os.makedirs(target_dir)\n",
        "#     print(f\"Directory '{target_dir}' was created.\")\n",
        "\n",
        "\n",
        "# for folder in os.listdir(source_dir):\n",
        "#     folder_path = os.path.join(source_dir, folder)\n",
        "#     print(folder_path)\n",
        "#     if os.path.isdir(folder_path):\n",
        "#         for file in os.listdir(folder_path):\n",
        "#             file_path = os.path.join(folder_path, file)\n",
        "#             print(file_path)\n",
        "#             shutil.copy(file_path, target_dir)"
      ],
      "metadata": {
        "id": "M_GmoOhbb-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !pip install labelme2coco\n",
        "# import labelme2coco\n",
        "\n",
        "# # LabelMeアノテーションと画像ファイルが含まれるディレクトリを設定\n",
        "# labelme_folder = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/concat_json\"\n",
        "\n",
        "# # エクスポートするディレクトリを設定\n",
        "# export_dir = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/coco_format\"\n",
        "\n",
        "# # トレーニングデータの分割率を設定\n",
        "# train_split_rate = 0.85\n",
        "\n",
        "# # LabelMeアノテーションをCOCO形式に変換\n",
        "# labelme2coco.convert(labelme_folder, export_dir, train_split_rate)"
      ],
      "metadata": {
        "id": "HPR155zyca9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ネストを減らすコード (要修正)"
      ],
      "metadata": {
        "id": "0cNyhbXsd4VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def convert_points_format(json_file_path):\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for shape in data['shapes']:\n",
        "        # ネストを1レベル減らします\n",
        "        shape['points'] = [point[0] for point in shape['points']]\n",
        "\n",
        "    with open(json_file_path, 'w') as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "\n",
        "# JSONファイルが保存されているディレクトリへのパス\n",
        "json_dir = \"/content/concat_json\"\n",
        "\n",
        "# JSONファイルの総数を取得\n",
        "total_files = sum([len(files) for r, d, files in os.walk(json_dir) if files])\n",
        "\n",
        "# ディレクトリ内の全てのJSONファイルに対して上記の関数を適用（tqdmを使用）\n",
        "with tqdm(total=total_files, desc=\"Converting JSON files\") as pbar:\n",
        "    for root, dirs, files in os.walk(json_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".json\"):\n",
        "                json_file_path = os.path.join(root, file)\n",
        "                convert_points_format(json_file_path)\n",
        "                pbar.update(1)"
      ],
      "metadata": {
        "id": "6bt34wEeckLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fe0874-4b14-4055-98e0-5f4a7cec59f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting JSON files: 100%|██████████| 10/10 [00:00<00:00, 1257.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install labelme2coco\n",
        "import labelme2coco\n",
        "\n",
        "# LabelMeアノテーションと画像ファイルが含まれるディレクトリを設定\n",
        "labelme_folder = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/ex_json\"\n",
        "\n",
        "# エクスポートするディレクトリを設定\n",
        "export_dir = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/coco_format\"\n",
        "\n",
        "# トレーニングデータの分割率を設定\n",
        "train_split_rate = 0.85\n",
        "\n",
        "# LabelMeアノテーションをCOCO形式に変換\n",
        "labelme2coco.convert(labelme_folder, export_dir, train_split_rate)"
      ],
      "metadata": {
        "id": "7o4cleqje5-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LabelMeアノテーションと画像ファイルが含まれるディレクトリを設定\n",
        "labelme_folder = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/concat_json\"\n",
        "\n",
        "# エクスポートするディレクトリを設定\n",
        "export_dir = \"/content/drive/MyDrive/AI_laboratory_course/MOBIUS/coco_format\"\n",
        "\n",
        "# トレーニングデータの分割率を設定\n",
        "train_split_rate = 0.85\n",
        "\n",
        "# フォルダ内のファイルリストを取得\n",
        "file_list = os.listdir(labelme_folder)\n",
        "\n",
        "# 進捗バーを表示してLabelMeアノテーションをCOCO形式に変換\n",
        "for file_name in tqdm(file_list, desc=\"Converting to COCO\"):\n",
        "    if file_name.endswith('.json'):\n",
        "        json_file_path = os.path.join(labelme_folder, file_name)\n",
        "        labelme2coco.convert(json_file_path, export_dir, train_split_rate)\n",
        "\n",
        "print(\"Conversion complete.\")"
      ],
      "metadata": {
        "id": "ltTxIjdtfCr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**YOLOv8形式のアノーテーションへの変換**"
      ],
      "metadata": {
        "id": "MuqoZneUf9Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def create_mask(image, color):\n",
        "    return np.all(image == color, axis=-1)\n",
        "\n",
        "def apply_mask(image, mask):\n",
        "    return np.where(mask, 255, 0)\n",
        "\n",
        "def extract_contours(img, _num_points):\n",
        "    contours = measure.find_contours(img, 0.5)\n",
        "    points_list = []\n",
        "\n",
        "    for contour in contours:\n",
        "        contour_length = len(contour)\n",
        "\n",
        "        if contour_length > _num_points:\n",
        "            step = contour_length // _num_points\n",
        "            selected_points = [contour[i * step] for i in range(_num_points)]\n",
        "        else:\n",
        "            selected_points = contour\n",
        "\n",
        "        points = [[float(point[1]), float(point[0])] for point in selected_points]\n",
        "        points_list.append(points)\n",
        "\n",
        "    return points_list\n",
        "\n",
        "def normalize_coordinates(points_list, img_width, img_height):\n",
        "    normalized_points_list = []\n",
        "    for points in points_list:\n",
        "        normalized_points = [[x / img_width, y / img_height] for x, y in points]\n",
        "        normalized_points_list.append(normalized_points)\n",
        "    return normalized_points_list\n",
        "\n",
        "def write_yolov8_annotation(points_list, class_index, img_width, img_height, file):\n",
        "    for points in points_list:\n",
        "        normalized_points = normalize_coordinates([points], img_width, img_height)[0]\n",
        "        line = f\"{class_index} \" + \" \".join(f\"{x} {y}\" for x, y in normalized_points) + \"\\n\"\n",
        "        file.write(line)\n",
        "\n",
        "def create_yolov8_text(mask_file, yolov8_annotation_file, num_points):\n",
        "    img = cv2.imread(mask_file, cv2.IMREAD_COLOR)\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    mask_blue = create_mask(img, [255, 0, 0])\n",
        "    mask_green = create_mask(img, [0, 255, 0])\n",
        "    mask_red = create_mask(img, [0, 0, 255])\n",
        "\n",
        "    mask_blue_green = np.logical_or(mask_blue, mask_green)\n",
        "    mask_all_colors = np.logical_or(mask_blue_green, mask_red)\n",
        "\n",
        "    img_blue = apply_mask(img, mask_blue)\n",
        "    img_blue_green = apply_mask(img, mask_blue_green)\n",
        "    img_all_colors = apply_mask(img, mask_all_colors)\n",
        "\n",
        "    points_list_blue = extract_contours(img_blue, num_points)\n",
        "    points_list_blue_green = extract_contours(img_blue_green, num_points)\n",
        "    points_list_all_colors = extract_contours(img_all_colors, num_points)\n",
        "\n",
        "    with open(yolov8_annotation_file, 'w') as file:\n",
        "        # クラスインデックス 0: 瞼\n",
        "        write_yolov8_annotation(points_list_blue, 0, img_width, img_height, file)\n",
        "        # クラスインデックス 1: 虹彩\n",
        "        write_yolov8_annotation(points_list_blue_green, 1, img_width, img_height, file)\n",
        "        # クラスインデックス 2: 瞳\n",
        "        write_yolov8_annotation(points_list_all_colors, 2, img_width, img_height, file)\n",
        "\n",
        "mask_file = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/masks/1/1_1i_Ll_2.png\"\n",
        "yolov8_annotation_file = \"/content/yolov8_annotation.txt\"\n",
        "num_points = 24\n",
        "\n",
        "create_yolov8_text(mask_file, yolov8_annotation_file, num_points)\n"
      ],
      "metadata": {
        "id": "mMMfQUGeb4-S"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv8フォーマットの確認（imgへの重ね合わせ）\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def read_yolov8_annotation(file_path, img_width, img_height):\n",
        "    annotations = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split()\n",
        "            class_index = int(parts[0])\n",
        "            points = [(float(parts[i]), float(parts[i + 1])) for i in range(1, len(parts), 2)]\n",
        "            # 正規化解除\n",
        "            points = [(x * img_width, y * img_height) for x, y in points]\n",
        "            annotations.append((class_index, points))\n",
        "    return annotations\n",
        "\n",
        "def plot_annotations(img, annotations):\n",
        "    plt.imshow(img)\n",
        "    for class_index, points in annotations:\n",
        "        xs, ys = zip(*points)\n",
        "        plt.plot(xs, ys, marker='o', markersize=2, linestyle='-')\n",
        "\n",
        "    plt.title(\"YOLOv8 Annotations\")\n",
        "    plt.show()\n",
        "\n",
        "# YOLOv8 アノーテーションファイルのパス\n",
        "yolov8_annotation_file = \"/content/yolov8_annotation.txt\"\n",
        "\n",
        "# 元の画像のパス\n",
        "img_path = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/images/1/1_1i_Ll_2.jpg\"\n",
        "img = cv2.imread(img_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "img_height, img_width = img.shape[:2]\n",
        "\n",
        "# アノーテーションの読み込みとプロット\n",
        "annotations = read_yolov8_annotation(yolov8_annotation_file, img_width, img_height)\n",
        "plot_annotations(img, annotations)\n"
      ],
      "metadata": {
        "id": "9nvecdzgfnbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCb9EaLOeZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/images/1\"\n",
        "masks_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/masks/1\"\n",
        "output_text_dir = \"/content/yolo8_annotation/\"\n",
        "num_points = 24\n",
        "\n",
        "if os.path.exists(output_text_dir):\n",
        "    shutil.rmtree(output_text_dir)\n",
        "os.makedirs(output_text_dir)\n",
        "\n",
        "filename_list = [os.path.splitext(f)[0] for f in os.listdir(images_dir) if os.path.isfile(os.path.join(images_dir, f))]\n",
        "\n",
        "for img in glob.glob(f\"{images_dir}/*\")[0:5]:\n",
        "    if os.path.splitext(os.path.basename(img))[0] in filename_list:\n",
        "        print(f\"processing... {os.path.splitext(os.path.basename(img))[0]}\")\n",
        "        txt_name = f\"{output_text_dir}/{os.path.splitext(os.path.basename(img))[0]}.txt\"\n",
        "        img = f\"{masks_dir}/{os.path.splitext(os.path.basename(img))[0]}.png\"\n",
        "        create_yolov8_text(img, txt_name, num_points)\n",
        "    else:\n",
        "        print(\"false\")"
      ],
      "metadata": {
        "id": "aoRJ4HLEj2zA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995b96b6-ad85-4b7b-ec17-91a3bcc37fc6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing... 1_1i_Ll_2\n",
            "processing... 1_1i_Ll_1\n",
            "processing... 1_1i_Lr_1\n",
            "processing... 1_1i_Ls_1\n",
            "processing... 1_1i_Lr_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 対応するマスク画像を使用してYOLOv8用のアノテーションテキストファイルを生成します。\n",
        "# images_dirの各サブディレクトリ(/1, /2など)に対して、同じ処理を適用します。\n",
        "# 同様に、masks_dirのサブディレクトリからマスク画像を取得し、\n",
        "# 出力テキストファイルをoutput_text_dirの同じサブディレクトリ構造に保存します。\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "\n",
        "images_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/images\"\n",
        "masks_dir = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/Sample_Mobius_dataset/masks\"\n",
        "output_text_dir = \"/content/yolo8_annotation/\"\n",
        "num_points = 24\n",
        "\n",
        "# 出力ディレクトリの初期化\n",
        "if os.path.exists(output_text_dir):\n",
        "    shutil.rmtree(output_text_dir)\n",
        "os.makedirs(output_text_dir)\n",
        "\n",
        "# images_dirのサブディレクトリを走査\n",
        "for subdir in os.listdir(images_dir):\n",
        "    images_subdir = os.path.join(images_dir, subdir)\n",
        "    masks_subdir = os.path.join(masks_dir, subdir)\n",
        "    output_subdir = os.path.join(output_text_dir, subdir)\n",
        "\n",
        "    if os.path.isdir(images_subdir) and os.path.isdir(masks_subdir):\n",
        "        os.makedirs(output_subdir, exist_ok=True)\n",
        "\n",
        "        # ファイル名リストを取得\n",
        "        filename_list = [os.path.splitext(f)[0] for f in os.listdir(images_subdir) if os.path.isfile(os.path.join(images_subdir, f))]\n",
        "\n",
        "        #for img_path in glob.glob(f\"{images_subdir}/*\"):\n",
        "        for img_path in glob.glob(f\"{images_subdir}/*\")[0:5]:\n",
        "            base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "            if base_name in filename_list:\n",
        "                print(f\"processing... {base_name}\")\n",
        "                txt_name = os.path.join(output_subdir, f\"{base_name}.txt\")\n",
        "                mask_img_path = os.path.join(masks_subdir, f\"{base_name}.png\")\n",
        "                create_yolov8_text(mask_img_path, txt_name, num_points)\n",
        "            else:\n",
        "                print(\"false\")\n"
      ],
      "metadata": {
        "id": "8ceK1D6-Knvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #################\n",
        "# # 北口くんの変換コード #\n",
        "# #################\n",
        "# from time import monotonic#多分いらない\n",
        "# #本体は以下から\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# #driveのマウント\n",
        "\n",
        "# import os\n",
        "# import json\n",
        "# import numpy as np\n",
        "\n",
        "# # jsonファイルの入っているフォルダのパスを指定\n",
        "# folder_path = '/content/drive/MyDrive/json-20240205T053811Z-001/json/4'\n",
        "\n",
        "\n",
        "\n",
        "# # フォルダ内の全ファイルにアクセス\n",
        "# for filename in os.listdir(folder_path):\n",
        "#     file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "#     # ファイルかどうかを確認\n",
        "#     if os.path.isfile(file_path):\n",
        "#         # ここでファイルに対する処理を行います\n",
        "#         print(f'ファイル名: {filename}, ファイルパス: {file_path}')\n",
        "\n",
        "\n",
        "#         with open(file_path,'r') as json_open:\n",
        "\n",
        "#             json_load = json.load(json_open)\n",
        "#             height = json_load['imageHeight']\n",
        "#             width = json_load['imageWidth']\n",
        "#             wh = np.array([width,height]) #割るための分母配列\n",
        "#             # テキストファイルを作成し、テキストを書き込む\n",
        "#             txtfile_name = f'{filename[:-5]}.txt'   #拡張子(.json)を削除\n",
        "#             txtfile_path = os.path.join('/content/drive/MyDrive/textfile_for_v8/4', txtfile_name)  # ファイルの名前と保存先のパスを指定\n",
        "\n",
        "#             for i in range(3):#eyelid,irys,pupilの三回分\n",
        "#                 points = [i for sublist in json_load['shapes'][i]['points'] for i in sublist]\n",
        "#                 pointsnp = np.array(points)\n",
        "#                 res = pointsnp/wh\n",
        "#                 monores = [element for row in res for element in row]#次元下げ\n",
        "\n",
        "#                 line = ' '.join(map(str, monores))\n",
        "\n",
        "\n",
        "#                 text_to_write = f'{i} {line}\\n'  #書く内容\n",
        "#                 with open(txtfile_path, 'a') as file:\n",
        "\n",
        "#                     file.write(text_to_write)\n",
        "\n",
        "\n",
        "\n",
        "#                 print(f'{file_path} ファイルを作成しました。{points} {wh}')#不要"
      ],
      "metadata": {
        "id": "DpEJn4jjkhaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**YOLOv8のtrain (鈴木くん)**\n",
        "\n",
        "・ badが含まれている画像およびannotationを削除する\n",
        "\n",
        "・ imagesのpngとannotationのテキストとを同じフォルダにして、trainとvalに分ける"
      ],
      "metadata": {
        "id": "TLJPf1k1fTCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#driveのマウント"
      ],
      "metadata": {
        "id": "KoAmfvOeK_T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807cd824-69e5-4d75-ae4b-42da72a2260a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/ultralytics\n",
        "%cd ultralytics\n",
        "#yolov8"
      ],
      "metadata": {
        "id": "PYuPV9j0fhVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b00e3ae3-3df9-4c56-d33f-7dd3960e48e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'ultralytics' already exists and is not an empty directory.\n",
            "/content/ultralytics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##簡単に確認"
      ],
      "metadata": {
        "id": "ToMGs5HtTegS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#本体\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8x.pt\")\n",
        "\n",
        "results = model(\"https://ultralytics.com/images/bus.jpg\",save=True)\n",
        "#Results saved to /content/ultralytics/runs/detect/predictの下に保存される"
      ],
      "metadata": {
        "id": "t_UKew5lfqUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "results = model('bus.jpg',save=True,task='segment')#/content/ultralytics/bus.jpgを利用\n",
        "#Results saved to /content/ultralytics/runs/segment/predictの下に保存される"
      ],
      "metadata": {
        "id": "vg-GjTEKgew0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trainig"
      ],
      "metadata": {
        "id": "Cx3FxsuzTh1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Badが含まれているファイルを削除\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "directory_list = [\"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/images/train\",\n",
        "                  \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/images/val\"]\n",
        "\n",
        "# ディレクトリ内のすべてのファイルとフォルダをチェック\n",
        "for directory in directory_list:\n",
        "    for item in os.listdir(directory):\n",
        "        # \"bad\"という文字列が含まれているか確認\n",
        "        if \"bad\" in item:\n",
        "            path_to_remove = os.path.join(directory, item)\n",
        "\n",
        "            # ファイルまたはフォルダを削除\n",
        "            if os.path.isfile(path_to_remove):\n",
        "                os.remove(path_to_remove)\n",
        "            elif os.path.isdir(path_to_remove):\n",
        "                shutil.rmtree(path_to_remove)\n",
        "            print(f\"Removed: {path_to_remove}\")"
      ],
      "metadata": {
        "id": "QQaXxExaTbDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YAMLファイルの作成\n",
        "\n",
        "%%writefile /content/eyesegmentation.yaml\n",
        "# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
        "path: /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8  # dataset root dir\n",
        "train: images/train  # train images (relative to 'path')\n",
        "val: images/val  # val images (relative to 'path')\n",
        "test: images/val # test images (optional)\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: eyelid\n",
        "  1: iris\n",
        "  2: pupil\n",
        "\n",
        "# Download script/URL (optional)\n",
        "download: https://ultralytics.com/assets/coco128-seg.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSkax_PWK855",
        "outputId": "322ede05-be6e-4f24-a6f4-0e91d9d440c6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/eyesegmentation.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #本体\n",
        "# import shutil\n",
        "\n",
        "# # コピー元ファイルのパス\n",
        "# source_file_path = '/content/drive/MyDrive/eyeditect-1.yaml'\n",
        "\n",
        "# # コピー先フォルダのパス\n",
        "# destination_folder = '/content/ultralytics/ultralytics/cfg/datasets'\n",
        "\n",
        "# # shutilモジュールのcopy関数を使用してファイルをコピー\n",
        "# shutil.copy(source_file_path, destination_folder)"
      ],
      "metadata": {
        "id": "LCpXut8th0y1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "#本体\n",
        "#nc = 3に変更\n",
        "# Load a model\n",
        "model = YOLO('yolov8n-seg.yaml').load('yolov8n-seg.pt')  # build from YAML and transfer weights"
      ],
      "metadata": {
        "id": "gE_MJLxNh1po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7c85c8-fd33-4267-fdd7-870a6827133f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transferred 417/417 items from pretrained weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result =  model.train(data='/content/eyesegmentation.yaml', epochs=30, imgsz=640)"
      ],
      "metadata": {
        "id": "2BBtFTuGiJSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe1b81d-8702-457b-8cdb-a204a2db90d7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.20 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.yaml, data=/content/eyesegmentation.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/ultralytics/runs/segment/train5\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1004665  ultralytics.nn.modules.head.Segment          [3, 32, 64, [64, 128, 256]]   \n",
            "YOLOv8n-seg summary: 261 layers, 3264201 parameters, 3264185 gradients\n",
            "\n",
            "Transferred 381/417 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/ultralytics/runs/segment/train5', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/labels/train... 340 images, 0 backgrounds, 0 corrupt: 100%|██████████| 340/340 [00:46<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/labels/val... 88 images, 0 backgrounds, 0 corrupt: 100%|██████████| 88/88 [00:14<00:00,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/labels/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/ultralytics/runs/segment/train5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/ultralytics/runs/segment/train5\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/30      5.42G      1.466      4.113      2.908      1.592         36        640: 100%|██████████| 22/22 [00:09<00:00,  2.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.977      0.159      0.548      0.385      0.961      0.151      0.479      0.228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/30      5.32G      1.106      2.339      1.335      1.309         17        640: 100%|██████████| 22/22 [00:05<00:00,  4.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.957      0.564      0.637       0.46      0.929      0.542      0.599       0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/30      5.27G      1.121      2.142      1.197      1.283         33        640: 100%|██████████| 22/22 [00:05<00:00,  4.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.907      0.591      0.797      0.509      0.919      0.571      0.794      0.393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/30       5.4G       1.03      1.985      1.027      1.223         28        640: 100%|██████████| 22/22 [00:05<00:00,  4.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.944      0.711      0.836      0.621      0.934      0.699       0.81      0.435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/30      5.42G      1.035      2.004     0.9962      1.235         25        640: 100%|██████████| 22/22 [00:05<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.849        0.7      0.895      0.611      0.807      0.683      0.848      0.445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/30      5.32G     0.9909      1.975     0.9434      1.208         31        640: 100%|██████████| 22/22 [00:05<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.832      0.786      0.855      0.616      0.829      0.792      0.856      0.473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/30      5.39G     0.9703      1.836     0.9013      1.188         26        640: 100%|██████████| 22/22 [00:05<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.865      0.852      0.878      0.644      0.847      0.852      0.874      0.514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/30      5.24G      0.951      1.771     0.8528      1.168         19        640: 100%|██████████| 22/22 [00:05<00:00,  4.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.891      0.879      0.911      0.665      0.909      0.864      0.898      0.523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/30      5.27G     0.9597      1.853     0.8555      1.178         21        640: 100%|██████████| 22/22 [00:05<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.927      0.865      0.904      0.672      0.894       0.83      0.866      0.491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/30      5.32G     0.9143      1.752     0.7984      1.175         25        640: 100%|██████████| 22/22 [00:05<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.894      0.913      0.897      0.643      0.882      0.902      0.883      0.501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/30      5.34G     0.9451      1.798     0.8016      1.187         32        640: 100%|██████████| 22/22 [00:05<00:00,  4.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.908      0.878      0.913      0.679      0.918      0.871      0.909      0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/30      5.38G     0.9105      1.672     0.7549      1.152         29        640: 100%|██████████| 22/22 [00:05<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.883      0.909      0.913      0.665      0.862      0.886      0.883      0.503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/30      5.25G     0.8769       1.68     0.7317      1.147         36        640: 100%|██████████| 22/22 [00:05<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.903      0.924      0.921       0.67      0.886      0.905      0.902      0.538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/30      5.38G      0.879      1.694     0.7071      1.136         37        640: 100%|██████████| 22/22 [00:05<00:00,  4.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.884       0.83      0.868      0.642      0.867      0.815      0.831       0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/30      5.31G     0.8568      1.605     0.6751      1.111         39        640: 100%|██████████| 22/22 [00:05<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.904      0.933      0.932       0.68      0.925      0.951      0.958      0.563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/30      5.28G     0.8465      1.566     0.6523      1.121         28        640: 100%|██████████| 22/22 [00:05<00:00,  4.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.918      0.919      0.917      0.671      0.925      0.924      0.928      0.529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/30      5.33G      0.827      1.524     0.6433      1.096         34        640: 100%|██████████| 22/22 [00:05<00:00,  4.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.908      0.917      0.912      0.678      0.905      0.917      0.916      0.555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/30      5.27G     0.8407      1.505     0.6256       1.11         23        640: 100%|██████████| 22/22 [00:05<00:00,  4.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.863      0.881      0.875      0.657       0.87      0.887      0.885      0.514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/30      5.43G      0.819      1.516     0.6244        1.1         36        640: 100%|██████████| 22/22 [00:05<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.914      0.902      0.914      0.696       0.92      0.905      0.926       0.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/30      5.39G     0.8044      1.533      0.599      1.089         24        640: 100%|██████████| 22/22 [00:05<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.938      0.943      0.948      0.708      0.934      0.939      0.944      0.565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/30      5.45G     0.8188      1.464     0.7228      1.131         12        640: 100%|██████████| 22/22 [00:09<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.949      0.942      0.958      0.705      0.945      0.939       0.95      0.567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/30      5.07G     0.7638      1.353     0.6298      1.114         12        640: 100%|██████████| 22/22 [00:05<00:00,  4.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.936       0.95      0.936      0.688      0.925      0.944      0.936      0.555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/30      5.07G     0.7617      1.381      0.608      1.089         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.914      0.913      0.934      0.696      0.908      0.911      0.933      0.566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/30      5.07G     0.7451      1.317     0.6005      1.089         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.938       0.89      0.935      0.702      0.909      0.919      0.937      0.561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/30      5.07G     0.7271      1.299     0.5853      1.099         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.932      0.943      0.953      0.698       0.94      0.947      0.958       0.57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/30      5.07G     0.7356      1.322     0.5615       1.08         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.905      0.931      0.927      0.694      0.917      0.917      0.927      0.559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/30      5.07G     0.7031      1.249     0.5406      1.063         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.932      0.936       0.94      0.717      0.936      0.939      0.947      0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/30      5.07G     0.7077       1.27     0.5298      1.064         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.925      0.909      0.913      0.705       0.93      0.913      0.924      0.566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/30      5.07G     0.6772      1.202     0.5181      1.053         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.964      0.924      0.946      0.709      0.952      0.952      0.954      0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/30      5.05G     0.6646      1.176     0.5043      1.036         12        640: 100%|██████████| 22/22 [00:04<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  3.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.926      0.902      0.914      0.702      0.926      0.909      0.917      0.563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "30 epochs completed in 0.060 hours.\n",
            "Optimizer stripped from /content/ultralytics/runs/segment/train5/weights/last.pt, 6.8MB\n",
            "Optimizer stripped from /content/ultralytics/runs/segment/train5/weights/best.pt, 6.8MB\n",
            "\n",
            "Validating /content/ultralytics/runs/segment/train5/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.20 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n-seg summary (fused): 195 layers, 3258649 parameters, 0 gradients\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         88        264      0.936      0.939      0.946      0.718      0.933      0.936      0.946      0.584\n",
            "                eyelid         88         88      0.994          1      0.995      0.852      0.994          1      0.995      0.752\n",
            "                  iris         88         88      0.994          1      0.995      0.882      0.983      0.989      0.985      0.592\n",
            "                 pupil         88         88       0.82      0.818      0.847       0.42       0.82      0.818      0.856       0.41\n",
            "Speed: 0.5ms preprocess, 2.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/segment/train5\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Inference trained model (鈴木くん)**"
      ],
      "metadata": {
        "id": "io6l7r-BiaIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#本体\n",
        "from ultralytics import YOLO\n",
        "# コンフィグは学習時と同じコンフィグでよい\n",
        "#model = YOLO(\"/Users/ryuka/workspace/Python/Image_AI/yolov8/ultralytics-main/ultralytics/cfg/models/v8/myyolov8.yaml\")  # build a new model from scratch\n",
        "\n",
        "# bestのモデルを使う,trainのウェイトのベストのパスを指定\n",
        "model = YOLO(\"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/sample_dataset_for_YOLOv8/last.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "#　画像のパスを指定\n",
        "img_path = \"/content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/testset(patient)/正常_小児_1.jpg\"\n",
        "results = model(img_path ,save=True, save_txt=True)  # predict on an image"
      ],
      "metadata": {
        "id": "ZjwKVV7EifpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd90a19-b45d-4983-f304-6ae9d0fe1658"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/Deep_learning/YOLACT_Mobius_ocular_dataset/testset(patient)/正常_小児_1.jpg: 640x640 1 eyelid, 1 iris, 1 pupil, 17.0ms\n",
            "Speed: 1.5ms preprocess, 17.0ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/segment/predict2\u001b[0m\n",
            "1 label saved to /content/ultralytics/runs/segment/predict2/labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# オブジェクトの種類を調べる\n",
        "for e in results[0].boxes.cls.cpu():\n",
        "  print(e, model.names[int(e)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu644_dBtvHE",
        "outputId": "a5452506-16a2-418e-b62b-ba0c35002429"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.) iris\n",
            "tensor(0.) eyelid\n",
            "tensor(2.) pupil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results[0].boxes"
      ],
      "metadata": {
        "id": "VFBjNKCctvJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0].masks"
      ],
      "metadata": {
        "id": "pLDUCeSVuMyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "aV-ITQRyuMz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, e in enumerate(results[0].masks.cpu().xy) :\n",
        "  pos = e.reshape((-1,1,2)).astype(np.int32)\n",
        "  cv2.polylines(img, [pos], isClosed=True, color= (255, 0, 32*i), thickness=2)\n",
        "  # cv2.fillPoly(img, [pos], color= (255, 0, 32*i))\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "ZSetsxx1uM1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1sx-Ir-juM3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oq7nxtCXtvLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = results[0] #検出した結果をresultに格納\n",
        "item=len(result.boxes) #検出したアイテムの数を出す\n",
        "print(item)\n",
        "box = result.boxes[0] #検出したアイテムのNo.0のBoxの情報\n",
        "\n",
        "class_id = result.names[box.cls[0].item()]\n",
        "cords = box.xyxy[0].tolist()\n",
        "cords = [round(x) for x in cords]\n",
        "conf = round(box.conf[0].item(), 2)\n",
        "\n",
        "print(\"Object type:\", class_id)\n",
        "print(\"Coordinates:(四捨五入後の値)\", cords)\n",
        "print(\"Probability:\", conf)"
      ],
      "metadata": {
        "id": "tueGZQPdilmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a9cd1b-1e23-45ed-fa47-1411c344584c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Object type: iris\n",
            "Coordinates:(四捨五入後の値) [170, 215, 410, 419]\n",
            "Probability: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(box)"
      ],
      "metadata": {
        "id": "ANIH10tcs04P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = results[0] #検出した結果をresultに格納\n",
        "idem=len(result.boxes) #検出したアイテムの数を出す\n",
        "print(idem)\n",
        "box = result.boxes[1] #検出したアイテムのNo.0のBoxの情報\n",
        "\n",
        "class_id = result.names[box.cls[0].item()]\n",
        "cords = box.xyxy[0].tolist()\n",
        "cords = [round(x) for x in cords]\n",
        "conf = round(box.conf[0].item(), 2)\n",
        "\n",
        "print(\"Object type:\", class_id)\n",
        "print(\"Coordinates:(四捨五入後の値)\", cords)\n",
        "print(\"Probability:\", conf)"
      ],
      "metadata": {
        "id": "xNWePP0Wip_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900cd6d3-a664-446d-cef7-12fd89e82b0c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Object type: eyelid\n",
            "Coordinates:(四捨五入後の値) [127, 210, 555, 436]\n",
            "Probability: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#本体\n",
        "result = results[0]\n",
        "itemnums = len(result.boxes)\n",
        "for i in range(itemnums):\n",
        "    box = result.boxes[i]\n",
        "    class_id = result.names[box.cls[0].item()]\n",
        "    cords = box.xyxy[0].tolist()\n",
        "    cords = [round(x) for x in cords]\n",
        "    if class_id == 'eyelid':\n",
        "        bunsi = cords[3]-cords[1]\n",
        "    elif class_id == 'iris':\n",
        "        bunbo = cords[2]-cords[0]\n",
        "ans = 1.2*bunsi/bunbo\n",
        "print('瞼裂径は{:.2f}'.format(ans),'㎝と予測されます！')"
      ],
      "metadata": {
        "id": "2BsigtPdiqlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9d95ce-c110-43af-fa8c-1340627e66ae"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "瞼裂径は1.13 ㎝と予測されます！\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "model =  YOLO('yolov8n-seg.pt')\n",
        "\n",
        "# Run inference on 'bus.jpg'\n",
        "results = model('/content/ultralytics/2019H-6_after_2.jpg')  # results list\n",
        "\n",
        "# Show the results\n",
        "for r in results:\n",
        "    im_array = r.plot(masks = True)  # plot a BGR numpy array of predictions\n",
        "    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
        "    im.show()  # show image\n",
        "    im.save('results2.jpg')  # save image"
      ],
      "metadata": {
        "id": "gVQK71xLiuHg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "aadc6cda-63a0-4481-ead7-03def33f49ff"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "/content/ultralytics/2019H-6_after_2.jpg does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-3566804f6e57>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Run inference on 'bus.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/ultralytics/2019H-6_after_2.jpg'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# results list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Show the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencapsulated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mResults\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     def track(\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for thread-safe inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# Setup source every time predict is called\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;31m# Check if save_dir/ label file exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36msetup_source\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         )\n\u001b[0;32m--> 230\u001b[0;31m         self.dataset = load_inference_source(\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvid_stride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         )\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/data/build.py\u001b[0m in \u001b[0;36mload_inference_source\u001b[0;34m(source, vid_stride, buffer)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadPilAndNumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvid_stride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvid_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# Attach source types to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ultralytics/ultralytics/data/loaders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, vid_stride)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# files (relative to *.txt file parent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{p} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mIMG_FORMATS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: /content/ultralytics/2019H-6_after_2.jpg does not exist"
          ]
        }
      ]
    }
  ]
}